{
  "goals": [
    "Register `bun nvst execute test-plan` in the CLI.",
    "Implement the command as a multi-step loop: iterate over test commands from `TP.json`'s `automatedTests` array, invoke the agent per test, verify results, track pass/fail, and support retries.",
    "Produce a persistent results file (`test-results.json`) tracking per-test outcomes."
  ],
  "userStories": [
    {
      "id": "US-001",
      "title": "Register Command",
      "description": "As a developer, I want to execute `bun nvst execute test-plan --agent <provider>` so that I can start the automated verification phase.",
      "acceptanceCriteria": [
        {
          "id": "US-001-AC01",
          "text": "`execute test-plan` is a valid command in the CLI."
        },
        {
          "id": "US-001-AC02",
          "text": "Command requires `--agent <provider>` flag, consistent with other agent-backed commands."
        },
        {
          "id": "US-001-AC03",
          "text": "Running `bun nvst execute test-plan` without required args prints usage to stderr."
        },
        {
          "id": "US-001-AC04",
          "text": "Command exits with an error if `phases.prototype.test_plan.status` in `state.json` is not `\"created\"`."
        },
        {
          "id": "US-001-AC05",
          "text": "Command exits with an error if `it_{iteration}_TP.json` does not exist in `.agents/flow/`."
        },
        {
          "id": "US-001-AC06",
          "text": "Typecheck / lint passes."
        }
      ]
    },
    {
      "id": "US-002",
      "title": "Automated Test Execution (Multi-Step Loop)",
      "description": "As a developer, I want the command to iterate over test commands and execute them so that each test is independently verified and results are tracked.",
      "acceptanceCriteria": [
        {
          "id": "US-002-AC01",
          "text": "Command reads `it_{iteration}_TP.json` from `.agents/flow/` and iterates over the `automatedTests` string array."
        },
        {
          "id": "US-002-AC02",
          "text": "Before execution starts, command validates that `it_{iteration}_TP.json` exists, parses as valid JSON, and contains a non-empty `automatedTests` array. If validation fails, command prints a human-readable error message to stderr and exits with code 1."
        },
        {
          "id": "US-002-AC03",
          "text": "For each entry in `automatedTests`, command invokes the agent and records the test command string, raw exit code, and normalized status (`passed`|`failed`) in `it_{iteration}_test-results.json`."
        },
        {
          "id": "US-002-AC04",
          "text": "Command logs structured output per test: `iteration=it_NNNNNN test_index=N attempt=N outcome=passed|failed`, matching the `create-prototype` logging pattern."
        },
        {
          "id": "US-002-AC05",
          "text": "Command supports `--iterations` and `--retry-on-fail` flags for retry behavior on failed tests."
        },
        {
          "id": "US-002-AC06",
          "text": "Command writes a results file (`it_{iteration}_test-results.json`) to `.agents/flow/` tracking per-test outcomes."
        },
        {
          "id": "US-002-AC07",
          "text": "Each test command execution is subject to a default timeout (60 seconds). If a test exceeds the timeout, it is recorded as `failed`."
        },
        {
          "id": "US-002-AC08",
          "text": "If `it_{iteration}_test-results.json` already exists, the command overwrites it with fresh results."
        }
      ]
    }
  ],
  "functionalRequirements": [
    {
      "id": "FR-1",
      "description": "Update `src/cli.ts` to include the `execute test-plan` command."
    },
    {
      "id": "FR-2",
      "description": "Create `src/commands/execute-test-plan.ts` to handle the command logic."
    },
    {
      "id": "FR-3",
      "description": "Invoke the agent via `invokeAgent()` from `src/agent.ts` for each test command in the loop."
    },
    {
      "id": "FR-4",
      "description": "Create `.agents/skills/execute-test-plan/SKILL.md` with the prompt template for test execution."
    }
  ]
}
